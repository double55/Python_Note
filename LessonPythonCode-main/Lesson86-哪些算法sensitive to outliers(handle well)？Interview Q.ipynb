{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第86讲 哪些算法sensitive to outliers(handle well)？Interview Q\n",
    "\n",
    "## Python学习：https://www.ixigua.com/home/77346806707?utm_source=xiguastudio\n",
    "\n",
    "## Python源文件及数据下载链接: https://github.com/rayc2020/LessonPythonCode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List of Machine Learning algorithms which are sensitive to outliers:\n",
    "1- Linear Regression\n",
    "2- Logistic Regression\n",
    "3- Support Vector Machine\n",
    "4- K- Nearest Neighbors\n",
    "5- K-Means Clustering\n",
    "6- Hierarchical Clustering\n",
    "7- Principal Component Analysis\n",
    "8- AdaBoost\n",
    "\n",
    "List of Machine Learning algorithms which are not sensitive to outliers:\n",
    "1- Decision Tree\n",
    "2- Random Forest\n",
    "3- XGBoost\n",
    "4- Naive Bayes\n",
    "5- DBSCAN\n",
    "6- NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have outliers, the best way is to use a clustering algorithm that can handle them.\n",
    "\n",
    "For example DBSCAN clustering is robust against outliers when you choose minpts large enough. Don't use k-means: the squared error approach is sensitive to outliers. But there are variants such as k-means-- for handling outliers.\n",
    "\n",
    "The neural network is resilient to the outliers' impact when the percentage-outliers in the test data is lower than 15%. This result is consistent with the result from the training set data.\n",
    "\n",
    "1) the multiple regression has only one single shot at fitting the data. Meanwhile the MLP has so many more opportunities to fit the data by varying the number of nodes and hidden layers to use to fit the data. This more flexible fitting mechanism should allow the MLP to underweight the impact of outliers (relative to either a Y or a X variable);\n",
    "\n",
    "2) MLPs activation functions typically use a Logit Regression mechanism (Sigmoid) or a Tangent Hyperbolic function (Tanh). The former generates intermediary outputs between 0 and 1 and the latter between -1 and +1. Those activation functions further enhance the capability of MLPs to deal with non-linear events and outliers.\n",
    "\n",
    "3) MLPs can incorporate regularization mechanisms. The latter should assist in resolving multicollinearity and reducing the impact of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Some thoughts about different classifiers:\n",
    "\n",
    "- both logistic regression and SVMs work great for linear problems, logistic regression may be preferable for very noisy data\n",
    "- naive Bayes can work better than logistic regression for small training sizes; also it is pretty fast, e.g., if you have a large multi-class problem, you'd only have to train one classifier whereas you'd have to use One-vs-Rest or One-vs-One with in SVMs or logistic regression\n",
    "- kernel SVM for nonlinear data\n",
    "- k-nearest neighbor can also work quite\n",
    "- Random forests & Extra Trees work well on linear and nonlinear problems\n",
    "\n",
    "My favorites (from a theoretical standpoint) are Neural Networks, but I at least try a handful of different algorithm (e.g., see the list above) when I build a model.\n",
    "\n",
    "Since this question is (impossible) to answer directly, let's maybe tackle the question from another angle and list the algorithms that are not so good:\n",
    "\n",
    "- clearly perceptrons: they are inferior to related classifiers such as adaptive linear neurons and logistic regression and there is no point in using them (okay, maybe speed can be a pro argument in very, very rare cases)\n",
    "\n",
    "- although k-nearest neighbors can be neat in certain cases where you have very complex functions that you want to approximate in a simple way, I wouldn't count it towards my \"favorite\" algorithms. Unless you have a very low-dimensional dataset you will likely suffer from the curse of dimensionality.\n",
    "\n",
    "- unless you need it for interpretability I would list decision trees (unless you want to visualize the decision rules); Random Forests and Extra Trees are \"better\"\n",
    "\n",
    "Now, the same rules apply for regression,\n",
    "\n",
    "- Start with Ordinary least squares regression\n",
    "- use LASSO, Ridge, or elastic net regression if high variance (overfitting) or collinearity is a problem\n",
    "- try RANSAC if you have many outliers\n",
    "- try Support Vector Regression (with a nonlinear kernel) or Random Forest regression if you have nonlinear data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In case of stock market crash or not prediction, false negative will be more important as if stock is predicted as not crash so you didn't withdraw money but actual it is crashed as result you lost money so here you are in loss(false negative), \n",
    "Second case :- false positive where you predict stock market crash so you withdraw money but actual it is not crash so result you were not able to earn profit becoz you withdraw money .\n",
    "Conclusion:- false negative is more important in case of stock market crash prediction.\n",
    "\n",
    "False negative is riskier;because if the person has infected with disease but predicted as negative it will be dangerous because it is contagious.\n",
    "\n",
    "As classifier assumes it as Spam and moved to trash, while it is not spam, then the user may misses the email and may causes a lot of trouble.\n",
    "While if it is spam but classifier failed to detect, then user will see the spam then it is not a big deal, user will delete it by one click."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
